{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Finding Good Features to solve problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 what are features?\n",
    "3.1.1 Features \n",
    "- attribute or charateristics\n",
    "- Information for specific purpose such as classification or regression.\n",
    "\n",
    "\n",
    "3.1.2 Formal definition of feature in the machine learning\n",
    "- A feature refers an independent input variable\n",
    "- A feature is typically constructed during the preprocessing manually or automatically\n",
    "- A feature often refers available constructed from the input variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.3 basic statistical feature\n",
    "- Mean and Variance\n",
    "- Mean: $\\frac{1}{n} * \\sum(X_i)$ \n",
    "- Variance: $\\frac{1}{n}*\\sum(X_i -X_{mean})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2Why are good features important?\n",
    "- We can reduce dimension of input data from the feature selection\n",
    "- Can help understanding of data, reduce of training time an execution time of algorithm.\n",
    "- improving prediction performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 How to extract features automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3.3.1 correlation criteria\n",
    " - Definition : statiscal technique which can show whether and how strongly pairs of variables are related\n",
    " - Mutual information of two random variables are defined by: $R(i) = Cov(X_i,Y) / \\sqrt{Var\\(x_i)\\ Var\\(y)}\\$\n",
    " - Euqation can calculate the relation between one feature and the target label in the dataset. \n",
    " - If close to -1, negative correlation, close to 0 then no correlation, close to 1, thenpositive correlation\n",
    " - A large value of square of correlation, feature and label are highly correlated.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.2 Single variable classifier\n",
    "- Given a feature $X_i$ and and output Y, compute the accuracy of a predictor.\n",
    "- True Positive Rate : $\\frac{{ num\\ of\\ predictied\\ positive }}{num\\ of\\ actual\\ positive }$\n",
    "- False positive Rate : $\\frac{{ num\\ of\\ predictied\\ positive }}{num\\ of\\ actual\\ negative }$ = $1- true\\ negative\\ rate$\n",
    "- The performance of individual classifier which trained with only one feature and label is used to compare the importance of feature\n",
    "- Especially the performance of single variable classifier is calculated by Receiver Operating Characteristics curve called ROC curve\n",
    "- The ROC curves typically feature true positive rate on the Y-axis and false positive rate on the X-axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.3 Variable subset selection\n",
    "- Definition: Goal is to find subset of input variables to maximize the classification performance \n",
    "- Finding the best subset of features is a NP-hard Problem\n",
    "\n",
    "3.3.4 Alternative Search Technique\n",
    "- Forward selection: Variables are progressively incorporated into larger and larger subsets\n",
    "- Backward selection: Variables are eliminated the least promising feature from a large set features \n",
    "> 1. Given the set of all features\n",
    "> 2. Assign weights of individual features\n",
    "> 3. Prune the feature with the smallest weight from the set of feature\n",
    "> 4. Recursively repeat this procedurre until you find the desired number of features with cross validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boruta Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We assume that a machine learning algorithm to optimize is given\n",
    "- The machine learning algorithm provides a numeric importance score to each feature after the training procedure. Boruta algorithm is a feature selection method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 모든 피쳐들을 복사해서 새로운 칼럼을 생성한다.\n",
    "> 2. 복사한 피쳐들 (섀도우 피쳐) 각각을 따로 섞는다.\n",
    "> 3. 섀도우 피쳐들에 대해서 랜덤 포레스트를 실행하고, Z score를 얻는다.\n",
    "> 4. 얻은 Z score 중에서 최댓값인 MSZA를 찾는다. (MSZA, Max Z-score among shadow attributes)\n",
    "> 5. 기존 피쳐들에 대해서 랜덤 포레스트를 실행하여 Z score를 얻는다.\n",
    "> 6. 통계적으로 유의한 수준에서 Z-score < MSZA인 경우, 해당 피쳐를 중요하지 않은 피쳐로 드랍한다.\n",
    "> 7. 통계적으로 유의한 수준에서 Z-score > MSZA인 경우, 해당 피쳐를 중요한 변수로 둔다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
